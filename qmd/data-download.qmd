# Log√¥nia: Netlogia Brasiliensis Growth Response Model in NetLogo

## Overview

This document provides a step-by-step guide to download the [WorldClim 2.1](https://worldclim.org/) data **for this model**. For a detailed pipeline on how to download the data, please refer to the [`LogoClim`](https://github.com/sustentarea/logoclim) model notebooks.

::: {.callout-important}
This process may take some time to complete. Please be patient.
:::

## Set the Environment

```{r}
library(beepr)
library(cli)
library(dplyr)
library(fs)
library(groomr) # github.com/sustentarea/groomr
library(here)
library(magrittr)
library(orbis) # github.com/sustentarea/orbis
library(purrr)
library(readr)
library(rutils) # github.com/sustentarea/rutils
library(rvest)
library(stringr)
library(tidyr)
library(zip)
```

```{r}
source(here("R", "wc_license.R"))
source(here("R", "wc_readme.R"))
```

## Set the Initial Variables

### Options

```{r}
options(cli.progress_show_after = 0)
```

```{r}
series <- "historical-monthly-weather-data"
```

```{r}
resolution <- "10m"
```

```{r}
model <- NULL
```

### Directories

Check the paths to the directories where the data will be stored.

```{r}
raw_data_dir <- here("data-raw")
```

```{r}
raw_data_wc_dir <- path(raw_data_dir, "worldclim")
```

```{r}
raw_data_wc_series_dir <- path(raw_data_wc_dir, series)
```

```{r}
raw_data_wc_series_res_dir <- path(
  raw_data_wc_dir,
  series,
  resolution |> str_replace_all("\\.", "\\-")
)
```

```{r}
dirs <- c(
  raw_data_dir, raw_data_wc_dir, raw_data_wc_series_dir,
  raw_data_wc_series_res_dir
)

for (i in dirs) {
  if (!dir_exists(i)) {
    dir_create(i, recurse = TRUE)
  }
}
```

## Scrape the Source

```{r}
html <-
  get_wc_url(series, resolution) |>
  read_html()
```

```{r}
urls <-
  html |>
  html_elements("a") |>
  html_attr("href") |>
  str_subset("geodata") %>%
  magrittr::extract(
    str_detect(
      basename(.),
      paste0("(?<=_)", resolution)
    )
  )
```

## Create the Metadata

```{r}
sizes <-
  urls |>
  map_dbl(
    .f = rutils::get_file_size,
    .progress = TRUE
  ) |>
  as_fs_bytes()

beep(1)
```

```{r}
metadata <-
  tibble(
    url = urls,
    file = basename(urls),
    size = sizes
  ) |>
  arrange(size) |>
  mutate(
  size_cum_sum =
      size |>
      replace_na() |>
      cumsum() |>
      as_fs_bytes()
  )

metadata
```

```{r}
metadata |> pull(size_cum_sum) |> last()
```

```{r}
metadata |>
  write_rds(
    path(raw_data_wc_series_res_dir, "metadata.rds")
  )
```

## Check for Errors

```{r}
{
  cli_alert_info(
    paste0(
      "{.strong {col_red(count_na(metadata$size))}} ",
      "url requests resulted in error."
    )
  )

  if (count_na(metadata$size) > 0) {
    cli_alert_info("Their file names are:")
    cli_li(metadata$file[is.na(metadata$size)])
  }
}
```

```{r}
if (any(is.na(metadata$size))) {
  paste0(
    "The following url requests resulted in error.",
    "\n\n",
    paste(metadata$url[is.na(metadata$size)], collapse = "\n")
  ) |>
    write_lines(here("qmd", "data-download.log"))
}
```

## Add LICENSE and README Files

```{r}
dir <- path(raw_data_wc_series_res_dir, "zip")

if (!dir_exists(dir)) {
  dir |> dir_create(recurse = TRUE)
}
```

```{r}
dirs <- c(
  raw_data_wc_dir,
  raw_data_wc_series_dir,
  raw_data_wc_series_res_dir,
  dir
)

for (i in dirs) {
  wc_license() |> write_lines(path(i, "LICENSE.md"))
}
```

```{r}
wc_readme() |>
  write_lines(path(raw_data_wc_dir, "README.md"))

wc_readme(series) |>
  write_lines(path(raw_data_wc_series_dir, "README.md"))

wc_readme(series, resolution) |>
  write_lines(path(raw_data_wc_series_res_dir, "README.md"))

wc_readme(series, resolution) |>
  write_lines(path(dir, "README.md"))
```

## Download the Files

```{r}
#| output: false

metadata |>
  pull(url) |>
  rutils::download_file(
    dir = dir,
    broken_links = TRUE
  )

beep(8)
```

If the download process was interrupted or not all files were downloaded, run the code chunk below to retry downloading only the missing files.

```{r}
#| eval: false
#| include: false

files <- dir |> dir_ls(type = "file")

metadata |>
  filter(!file %in% basename(files)) |>
  pull(url) |>
  rutils::download_file(dir = dir, broken_links = TRUE)

beep(8)
```

## Unzip the Files

```{r}
zip_dir <- path(raw_data_wc_series_res_dir, "zip")
```

```{r}
tif_dir <- path(raw_data_wc_series_res_dir, "tif")

if (!dir_exists(tif_dir)) tif_dir |> dir_create(recurse = TRUE)
```

```{r}
if (dir_exists(zip_dir)) {
  accompanying_files <- c("README.md", "LICENSE.md")

  for (i in accompanying_files) {
    if (file_exists(path(zip_dir, i))) {
      file_copy(
        path(zip_dir, i),
        path(tif_dir, i),
        overwrite = TRUE
      )
    }
  }
}
```

```{r}
#| output: false

if (dir_exists(zip_dir)) {
  zip_files <-
    zip_dir |>
    dir_ls(type = "file", regexp = "zip$")

  cli_progress_bar(
      name = "Unzipping data",
      total = length(zip_files),
      clear = FALSE
    )

  for (i in zip_files) {
    i |>
      zip::unzip(
        overwrite = TRUE,
        exdir = tif_dir
      )

    cli_progress_update()
  }

  cli_progress_done()

  beep(1)
}
```

```{r}
if (dir_exists(zip_dir)) zip_dir |> dir_delete()
```
